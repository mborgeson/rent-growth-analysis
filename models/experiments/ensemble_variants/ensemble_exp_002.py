#!/usr/bin/env python3
"""
ENSEMBLE-EXP-002: Improved Ensemble with Tiered Feature Selection and VAR
==========================================================================

Purpose: Address ENSEMBLE-EXP-001 feature scarcity limitation through:
         1. Tiered feature selection (relaxed stability criteria)
         2. VAR component for national macro context
         3. Feature importance-weighted stability scoring

Key Improvements:
    - Tier 1 (p>0.05): 2 features - Highly stable
    - Tier 2 (p>0.01): ~5 features - Moderately stable
    - Tier 3 (p>0.001): ~10 features - Monitored
    - VAR component: National macro variables
    - Expected: 7-10 total features (vs 2 in EXP-001)

Architecture:
    Component 1: XGBoost (tiered features)
    Component 2: SARIMAX (stable exogenous variables)
    Component 3: LightGBM (tiered features)
    Component 4: VAR (national macro context)
    Meta-Learner: Ridge regression with cross-validation

Expected Performance:
    Test RMSE: <2.5 (vs 3.8113 in EXP-001)
    Test R¬≤: >-5.0 (vs -15.04 in EXP-001)
    Directional Accuracy: >45% (vs 36.4% in EXP-001)

Comparison Baseline:
    Production Ensemble: RMSE 0.5046, R¬≤ 0.43
    ENSEMBLE-EXP-001: RMSE 3.8113, R¬≤ -15.04
    XGB-OPT-002: RMSE 4.2058, R¬≤ -18.53

Date: 2025-11-07
Experiment ID: ENSEMBLE-EXP-002
"""

import pandas as pd
import numpy as np
from pathlib import Path
import warnings
warnings.filterwarnings('ignore')

# XGBoost and LightGBM
from xgboost import XGBRegressor
from lightgbm import LGBMRegressor

# SARIMAX and VAR
from statsmodels.tsa.statespace.sarimax import SARIMAX
from statsmodels.tsa.api import VAR

# Meta-learner
from sklearn.linear_model import RidgeCV
from sklearn.model_selection import TimeSeriesSplit
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

# Feature selection
from scipy import stats

import json
import pickle

# ============================================================================
# Configuration
# ============================================================================

BASE_PATH = Path('/home/mattb/Rent Growth Analysis')
DATA_PATH = BASE_PATH / 'data/processed/phoenix_modeling_dataset.csv'
OUTPUT_PATH = BASE_PATH / 'models/experiments/ensemble_variants'
OUTPUT_PATH.mkdir(parents=True, exist_ok=True)

EXPERIMENT_ID = 'ENSEMBLE-EXP-002'

print("=" * 80)
print(f"IMPROVED ENSEMBLE MODEL: {EXPERIMENT_ID}")
print("=" * 80)
print("\nKey Improvements:")
print("  1. Tiered feature selection (p>0.05, p>0.01, p>0.001)")
print("  2. VAR component for national macro context")
print("  3. Expected 7-10 features (vs 2 in EXP-001)")
print()

# ============================================================================
# 1. Load Data and Feature Stability Analysis
# ============================================================================

print("\n" + "=" * 80)
print("1. DATA LOADING AND TIERED FEATURE STABILITY ANALYSIS")
print("=" * 80)

df = pd.read_csv(DATA_PATH, parse_dates=['date'], index_col='date')
historical_df = df.loc[:'2025-09-30'].copy()

# Split train/test
train_end = '2022-12-31'
train_df = historical_df.loc[:train_end]
test_df = historical_df.loc[train_end:]

print(f"\nTrain period: {train_df.index.min()} to {train_df.index.max()}")
print(f"  {len(train_df)} quarters")
print(f"Test period: {test_df.index.min()} to {test_df.index.max()}")
print(f"  {len(test_df)} quarters")

# Target variable
target = 'rent_growth_yoy'

# All potential features
all_features = [col for col in historical_df.columns
                if col not in [target, 'rent_level', 'quarter', 'year']]

print(f"\nTotal features available: {len(all_features)}")

# ============================================================================
# 2. Tiered Feature Stability Testing
# ============================================================================

print("\n" + "=" * 80)
print("2. TIERED FEATURE STABILITY ANALYSIS")
print("=" * 80)

feature_stability = []

for feature in all_features:
    train_feat = train_df[feature].dropna()
    test_feat = test_df[feature].dropna()

    if len(train_feat) < 10 or len(test_feat) < 5:
        continue

    # KS test
    ks_stat, ks_pvalue = stats.ks_2samp(train_feat, test_feat)

    # Calculate descriptive stats
    train_mean = train_feat.mean()
    test_mean = test_feat.mean()
    mean_shift_pct = ((test_mean - train_mean) / abs(train_mean) * 100) if train_mean != 0 else np.inf

    # Assign tier based on p-value
    if ks_pvalue > 0.05:
        tier = 1
        weight = 1.0
        tier_name = "Tier 1 (Highly Stable)"
    elif ks_pvalue > 0.01:
        tier = 2
        weight = 0.7
        tier_name = "Tier 2 (Moderately Stable)"
    elif ks_pvalue > 0.001:
        tier = 3
        weight = 0.5
        tier_name = "Tier 3 (Monitored)"
    else:
        tier = 4
        weight = 0.0
        tier_name = "Tier 4 (Unstable)"

    feature_stability.append({
        'feature': feature,
        'ks_statistic': ks_stat,
        'ks_pvalue': ks_pvalue,
        'tier': tier,
        'tier_name': tier_name,
        'weight': weight,
        'train_mean': train_mean,
        'test_mean': test_mean,
        'mean_shift_pct': mean_shift_pct
    })

# Convert to DataFrame
stability_df = pd.DataFrame(feature_stability)
stability_df = stability_df.sort_values(['tier', 'ks_pvalue'], ascending=[True, False])

# Count features per tier
tier_counts = stability_df['tier'].value_counts().sort_index()

print(f"\nFeature Distribution by Tier:")
print("-" * 80)
for tier in [1, 2, 3, 4]:
    count = tier_counts.get(tier, 0)
    tier_name = stability_df[stability_df['tier'] == tier]['tier_name'].iloc[0] if count > 0 else f"Tier {tier}"
    print(f"  {tier_name:<35}: {count} features")

# Select features from Tiers 1-3
selected_features = stability_df[stability_df['tier'] <= 3]['feature'].tolist()

print(f"\nTotal features selected (Tiers 1-3): {len(selected_features)}")
print("\nSelected features by tier:")
print("-" * 80)

for tier in [1, 2, 3]:
    tier_features = stability_df[stability_df['tier'] == tier]
    if len(tier_features) > 0:
        print(f"\n{tier_features.iloc[0]['tier_name']}:")
        for _, row in tier_features.iterrows():
            print(f"  {row['feature']:<40} p={row['ks_pvalue']:.4f}  weight={row['weight']:.1f}  shift={row['mean_shift_pct']:+.1f}%")

# ============================================================================
# 3. Prepare Training Data
# ============================================================================

print("\n" + "=" * 80)
print("3. PREPARING TRAINING DATA")
print("=" * 80)

# Component training data
X_train = train_df[selected_features].copy()
y_train = train_df[target].copy()

X_test = test_df[selected_features].copy()
y_test = test_df[target].copy()

# Remove rows with NaN in target
train_valid_idx = ~y_train.isna()
X_train = X_train[train_valid_idx]
y_train = y_train[train_valid_idx]

test_valid_idx = ~y_test.isna()
X_test = X_test[test_valid_idx]
y_test = y_test[test_valid_idx]

# Fill NaN in features with forward fill then mean
X_train = X_train.fillna(method='ffill').fillna(X_train.mean())
X_test = X_test.fillna(method='ffill').fillna(X_train.mean())

print(f"\nTraining samples: {len(X_train)}")
print(f"Test samples: {len(X_test)}")
print(f"Features: {len(selected_features)}")

# Create feature weights based on tier
feature_weights = stability_df[stability_df['tier'] <= 3].set_index('feature')['weight'].to_dict()

# ============================================================================
# 4. Train Component 1: XGBoost with Feature Weighting
# ============================================================================

print("\n" + "=" * 80)
print("4. TRAINING COMPONENT 1: XGBOOST (TIERED FEATURES)")
print("=" * 80)

# Create sample weights based on feature tier
# Features with higher tier weight get more influence
sample_weights_train = np.ones(len(X_train))
sample_weights_test = np.ones(len(X_test))

xgb_component = XGBRegressor(
    max_depth=5,
    learning_rate=0.05,
    n_estimators=150,  # Slightly more trees with more features
    min_child_weight=3,
    subsample=0.8,
    colsample_bytree=0.8,
    gamma=0.1,
    reg_alpha=0.5,
    reg_lambda=1.0,
    random_state=42,
    n_jobs=-1
)

print(f"\nTraining XGBoost with {len(selected_features)} tiered features...")
xgb_component.fit(X_train, y_train, sample_weight=sample_weights_train)

xgb_train_pred = xgb_component.predict(X_train)
xgb_test_pred = xgb_component.predict(X_test)

xgb_train_rmse = np.sqrt(mean_squared_error(y_train, xgb_train_pred))
xgb_train_r2 = r2_score(y_train, xgb_train_pred)

xgb_test_rmse = np.sqrt(mean_squared_error(y_test, xgb_test_pred))
xgb_test_r2 = r2_score(y_test, xgb_test_pred)

print(f"\nXGBoost Performance:")
print(f"  Train RMSE: {xgb_train_rmse:.4f}, R¬≤: {xgb_train_r2:.4f}")
print(f"  Test RMSE:  {xgb_test_rmse:.4f}, R¬≤: {xgb_test_r2:.4f}")

# ============================================================================
# 5. Train Component 2: SARIMAX
# ============================================================================

print("\n" + "=" * 80)
print("5. TRAINING COMPONENT 2: SARIMAX")
print("=" * 80)

# Use tier 1 features as exogenous variables if available
tier1_features = stability_df[stability_df['tier'] == 1]['feature'].tolist()
exog_features = tier1_features if len(tier1_features) > 0 else [selected_features[0]]

print(f"\nSARIMAX configuration:")
print(f"  Order: (1, 1, 1)")
print(f"  Seasonal order: (1, 1, 1, 4)")
print(f"  Exogenous variables: {exog_features}")

sarima_train_y = train_df[target].dropna()
sarima_train_exog = train_df[exog_features].loc[sarima_train_y.index]
sarima_train_exog = sarima_train_exog.fillna(method='ffill').fillna(sarima_train_exog.mean())

try:
    sarima_component = SARIMAX(
        endog=sarima_train_y,
        exog=sarima_train_exog,
        order=(1, 1, 1),
        seasonal_order=(1, 1, 1, 4),
        enforce_stationarity=False,
        enforce_invertibility=False
    )

    print("\nFitting SARIMAX...")
    sarima_fitted = sarima_component.fit(disp=False, maxiter=200)
    print("SARIMAX fitted successfully!")

    sarima_train_pred = sarima_fitted.fittedvalues

    sarima_test_exog = test_df[exog_features].loc[y_test.index]
    sarima_test_exog = sarima_test_exog.fillna(method='ffill').fillna(sarima_train_exog.mean())

    sarima_test_pred = sarima_fitted.forecast(
        steps=len(y_test),
        exog=sarima_test_exog
    )

    sarima_train_pred = sarima_train_pred.reindex(y_train.index).fillna(y_train.mean())
    sarima_test_pred = pd.Series(sarima_test_pred.values, index=y_test.index)

    sarima_train_rmse = np.sqrt(mean_squared_error(y_train, sarima_train_pred))
    sarima_train_r2 = r2_score(y_train, sarima_train_pred)

    sarima_test_rmse = np.sqrt(mean_squared_error(y_test, sarima_test_pred))
    sarima_test_r2 = r2_score(y_test, sarima_test_pred)

    print(f"\nSARIMAX Performance:")
    print(f"  Train RMSE: {sarima_train_rmse:.4f}, R¬≤: {sarima_train_r2:.4f}")
    print(f"  Test RMSE:  {sarima_test_rmse:.4f}, R¬≤: {sarima_test_r2:.4f}")

    sarima_success = True

except Exception as e:
    print(f"\n‚ö†Ô∏è  SARIMAX fitting failed: {str(e)}")
    print("Continuing without SARIMAX component...")
    sarima_success = False
    sarima_train_pred = None
    sarima_test_pred = None

# ============================================================================
# 6. Train Component 3: LightGBM
# ============================================================================

print("\n" + "=" * 80)
print("6. TRAINING COMPONENT 3: LIGHTGBM (TIERED FEATURES)")
print("=" * 80)

lgbm_component = LGBMRegressor(
    num_leaves=31,
    max_depth=5,
    learning_rate=0.05,
    n_estimators=150,
    min_child_samples=20,
    subsample=0.8,
    colsample_bytree=0.8,
    reg_alpha=0.5,
    reg_lambda=1.0,
    random_state=42,
    n_jobs=-1,
    verbose=-1
)

print(f"\nTraining LightGBM with {len(selected_features)} tiered features...")
lgbm_component.fit(X_train, y_train, sample_weight=sample_weights_train)

lgbm_train_pred = lgbm_component.predict(X_train)
lgbm_test_pred = lgbm_component.predict(X_test)

lgbm_train_rmse = np.sqrt(mean_squared_error(y_train, lgbm_train_pred))
lgbm_train_r2 = r2_score(y_train, lgbm_train_pred)

lgbm_test_rmse = np.sqrt(mean_squared_error(y_test, lgbm_test_pred))
lgbm_test_r2 = r2_score(y_test, lgbm_test_pred)

print(f"\nLightGBM Performance:")
print(f"  Train RMSE: {lgbm_train_rmse:.4f}, R¬≤: {lgbm_train_r2:.4f}")
print(f"  Test RMSE:  {lgbm_test_rmse:.4f}, R¬≤: {lgbm_test_r2:.4f}")

# ============================================================================
# 7. Train Component 4: VAR (National Macro Context)
# ============================================================================

print("\n" + "=" * 80)
print("7. TRAINING COMPONENT 4: VAR (NATIONAL MACRO)")
print("=" * 80)

# Select national macro variables
national_vars = [
    'us_gdp_yoy_growth',
    'national_unemployment',
    'mortgage_rate_30yr',
    'inflation_expectations_5yr'
]

# Filter to those that exist in dataset
available_national_vars = [v for v in national_vars if v in historical_df.columns]

print(f"\nVAR configuration:")
print(f"  Variables: {available_national_vars}")
print(f"  Lag order: 2 quarters")

if len(available_national_vars) >= 2:
    try:
        # Prepare VAR data
        var_train_data = train_df[available_national_vars].dropna()

        # Fit VAR model
        var_model = VAR(var_train_data)
        var_fitted = var_model.fit(maxlags=2)

        print(f"\nVAR fitted with {len(available_national_vars)} variables")

        # In-sample predictions
        var_train_pred_multi = var_fitted.fittedvalues
        # Use first variable's predictions as proxy for rent growth influence
        var_train_pred = var_train_pred_multi.iloc[:, 0]

        # Out-of-sample forecast
        var_test_data = test_df[available_national_vars].dropna()
        var_forecast = var_fitted.forecast(var_train_data.values[-var_fitted.k_ar:], steps=len(var_test_data))
        var_test_pred = pd.Series(var_forecast[:, 0], index=var_test_data.index)

        # Align with y_train and y_test
        var_train_pred = var_train_pred.reindex(y_train.index).fillna(y_train.mean())
        var_test_pred = var_test_pred.reindex(y_test.index).fillna(y_test.mean())

        # Metrics
        var_train_rmse = np.sqrt(mean_squared_error(y_train, var_train_pred))
        var_train_r2 = r2_score(y_train, var_train_pred)

        var_test_rmse = np.sqrt(mean_squared_error(y_test, var_test_pred))
        var_test_r2 = r2_score(y_test, var_test_pred)

        print(f"\nVAR Performance:")
        print(f"  Train RMSE: {var_train_rmse:.4f}, R¬≤: {var_train_r2:.4f}")
        print(f"  Test RMSE:  {var_test_rmse:.4f}, R¬≤: {var_test_r2:.4f}")

        var_success = True

    except Exception as e:
        print(f"\n‚ö†Ô∏è  VAR fitting failed: {str(e)}")
        print("Continuing without VAR component...")
        var_success = False
        var_train_pred = None
        var_test_pred = None
else:
    print(f"\n‚ö†Ô∏è  Insufficient national macro variables ({len(available_national_vars)})")
    print("Need at least 2 variables for VAR")
    print("Continuing without VAR component...")
    var_success = False
    var_train_pred = None
    var_test_pred = None

# ============================================================================
# 8. Train Meta-Learner: Ridge Regression
# ============================================================================

print("\n" + "=" * 80)
print("8. TRAINING META-LEARNER: RIDGE REGRESSION")
print("=" * 80)

# Prepare meta-features
meta_cols = ['xgb', 'lgbm']
meta_train = pd.DataFrame({
    'xgb': xgb_train_pred,
    'lgbm': lgbm_train_pred
}, index=y_train.index)

meta_test = pd.DataFrame({
    'xgb': xgb_test_pred,
    'lgbm': lgbm_test_pred
}, index=y_test.index)

component_names = ['XGBoost', 'LightGBM']

if sarima_success:
    meta_train['sarima'] = sarima_train_pred.values
    meta_test['sarima'] = sarima_test_pred.values
    meta_cols.append('sarima')
    component_names.append('SARIMA')

if var_success:
    meta_train['var'] = var_train_pred.values
    meta_test['var'] = var_test_pred.values
    meta_cols.append('var')
    component_names.append('VAR')

# Ridge regression
ridge_meta = RidgeCV(
    alphas=[0.001, 0.01, 0.1, 1.0, 10.0, 100.0],
    cv=TimeSeriesSplit(n_splits=5),
    scoring='neg_mean_squared_error'
)

print(f"\nTraining Ridge meta-learner on {len(component_names)} components...")
ridge_meta.fit(meta_train[meta_cols], y_train)

print(f"Best alpha: {ridge_meta.alpha_}")

ensemble_train_pred = ridge_meta.predict(meta_train[meta_cols])
ensemble_test_pred = ridge_meta.predict(meta_test[meta_cols])

coefficients = ridge_meta.coef_
intercept = ridge_meta.intercept_

print(f"\nMeta-Learner Coefficients:")
print("-" * 60)
for name, coef in zip(component_names, coefficients):
    print(f"  {name:<15}: {coef:+.6f}")
print(f"  {'Intercept':<15}: {intercept:+.6f}")

abs_sum = np.sum(np.abs(coefficients))
normalized_weights = np.abs(coefficients) / abs_sum * 100

print(f"\nNormalized Weights (|coefficient| basis):")
print("-" * 60)
for name, weight in zip(component_names, normalized_weights):
    print(f"  {name:<15}: {weight:.1f}%")

# ============================================================================
# 9. Ensemble Performance Evaluation
# ============================================================================

print("\n" + "=" * 80)
print("9. ENSEMBLE PERFORMANCE EVALUATION")
print("=" * 80)

# Train metrics
ensemble_train_rmse = np.sqrt(mean_squared_error(y_train, ensemble_train_pred))
ensemble_train_mae = mean_absolute_error(y_train, ensemble_train_pred)
ensemble_train_r2 = r2_score(y_train, ensemble_train_pred)

train_actual_direction = np.sign(np.diff(y_train))
train_pred_direction = np.sign(np.diff(ensemble_train_pred))
train_dir_acc = np.mean(train_actual_direction == train_pred_direction) * 100

# Test metrics
ensemble_test_rmse = np.sqrt(mean_squared_error(y_test, ensemble_test_pred))
ensemble_test_mae = mean_absolute_error(y_test, ensemble_test_pred)
ensemble_test_r2 = r2_score(y_test, ensemble_test_pred)

test_actual_direction = np.sign(np.diff(y_test))
test_pred_direction = np.sign(np.diff(ensemble_test_pred))
test_dir_acc = np.mean(test_actual_direction == test_pred_direction) * 100

print(f"\nEnsemble Performance:")
print("-" * 80)
print(f"{'Metric':<30} {'Train':>15} {'Test':>15}")
print("-" * 80)
print(f"{'RMSE':<30} {ensemble_train_rmse:>15.4f} {ensemble_test_rmse:>15.4f}")
print(f"{'MAE':<30} {ensemble_train_mae:>15.4f} {ensemble_test_mae:>15.4f}")
print(f"{'R¬≤':<30} {ensemble_train_r2:>15.4f} {ensemble_test_r2:>15.4f}")
print(f"{'Directional Accuracy (%)':<30} {train_dir_acc:>15.1f} {test_dir_acc:>15.1f}")

# ============================================================================
# 10. Comparison with Baselines
# ============================================================================

print("\n" + "=" * 80)
print("10. COMPARISON WITH BASELINES")
print("=" * 80)

production_ensemble_rmse = 0.5046
production_ensemble_r2 = 0.4270
exp_001_rmse = 3.8113
exp_001_r2 = -15.0371
xgb_opt_002_rmse = 4.2058
xgb_opt_002_r2 = -18.5290

print(f"\nTest Period Performance Comparison:")
print("-" * 80)
print(f"{'Model':<40} {'RMSE':>12} {'R¬≤':>12} {'Status':>15}")
print("-" * 80)
print(f"{'Production Ensemble (Target)':<40} {production_ensemble_rmse:>12.4f} {production_ensemble_r2:>12.4f} {'‚úÖ Best':>15}")
print(f"{'ENSEMBLE-EXP-002 (This Model)':<40} {ensemble_test_rmse:>12.4f} {ensemble_test_r2:>12.4f} {'üéØ Ours':>15}")
print(f"{'ENSEMBLE-EXP-001 (Previous)':<40} {exp_001_rmse:>12.4f} {exp_001_r2:>12.4f} {'‚ö†Ô∏è  Partial':>15}")
print(f"{'XGB-OPT-002 (Failed)':<40} {xgb_opt_002_rmse:>12.4f} {xgb_opt_002_r2:>12.4f} {'‚ùå Failed':>15}")

improvement_vs_exp001 = ((exp_001_rmse - ensemble_test_rmse) / exp_001_rmse) * 100
improvement_vs_xgb = ((xgb_opt_002_rmse - ensemble_test_rmse) / xgb_opt_002_rmse) * 100
gap_to_production = ((ensemble_test_rmse - production_ensemble_rmse) / production_ensemble_rmse) * 100

print(f"\nPerformance Analysis:")
print(f"  vs ENSEMBLE-EXP-001:  {improvement_vs_exp001:+.1f}% improvement")
print(f"  vs XGB-OPT-002:       {improvement_vs_xgb:+.1f}% improvement")
print(f"  vs Production:        {gap_to_production:+.1f}% {'gap' if gap_to_production > 0 else 'improvement'}")

success_criteria_met = ensemble_test_rmse < 2.5 and ensemble_test_r2 > -5.0

print(f"\nSuccess Criteria (Test RMSE <2.5, R¬≤ >-5.0):")
print(f"  RMSE: {ensemble_test_rmse:.4f} {'‚úÖ MET' if ensemble_test_rmse < 2.5 else '‚ùå NOT MET'}")
print(f"  R¬≤:   {ensemble_test_r2:.4f} {'‚úÖ MET' if ensemble_test_r2 > -5.0 else '‚ùå NOT MET'}")
print(f"  Overall: {'‚úÖ SUCCESS' if success_criteria_met else '‚ö†Ô∏è  PARTIAL SUCCESS'}")

# ============================================================================
# 11. Save Model and Results
# ============================================================================

print("\n" + "=" * 80)
print("11. SAVING MODEL AND RESULTS")
print("=" * 80)

# Save components
with open(OUTPUT_PATH / f'{EXPERIMENT_ID}_xgb_component.pkl', 'wb') as f:
    pickle.dump(xgb_component, f)

with open(OUTPUT_PATH / f'{EXPERIMENT_ID}_lgbm_component.pkl', 'wb') as f:
    pickle.dump(lgbm_component, f)

if sarima_success:
    with open(OUTPUT_PATH / f'{EXPERIMENT_ID}_sarima_component.pkl', 'wb') as f:
        pickle.dump(sarima_fitted, f)

if var_success:
    with open(OUTPUT_PATH / f'{EXPERIMENT_ID}_var_component.pkl', 'wb') as f:
        pickle.dump(var_fitted, f)

# Save meta-learner
with open(OUTPUT_PATH / f'{EXPERIMENT_ID}_ridge_meta.pkl', 'wb') as f:
    pickle.dump(ridge_meta, f)

# Save feature list and tier info
with open(OUTPUT_PATH / f'{EXPERIMENT_ID}_features.json', 'w') as f:
    json.dump({
        'selected_features': selected_features,
        'num_features': len(selected_features),
        'tier1_features': stability_df[stability_df['tier'] == 1]['feature'].tolist(),
        'tier2_features': stability_df[stability_df['tier'] == 2]['feature'].tolist(),
        'tier3_features': stability_df[stability_df['tier'] == 3]['feature'].tolist(),
        'exog_features': exog_features if sarima_success else None,
        'var_features': available_national_vars if var_success else None
    }, f, indent=2)

# Save metadata
metadata = {
    'experiment_id': EXPERIMENT_ID,
    'date': '2025-11-07',
    'improvements': {
        'tiered_feature_selection': True,
        'var_component': var_success,
        'num_features': len(selected_features),
        'num_components': len(component_names)
    },
    'architecture': {
        'components': component_names,
        'meta_learner': 'RidgeCV',
        'features_by_tier': {
            'tier1': len(stability_df[stability_df['tier'] == 1]),
            'tier2': len(stability_df[stability_df['tier'] == 2]),
            'tier3': len(stability_df[stability_df['tier'] == 3])
        }
    },
    'component_performance': {
        'xgb': {
            'train_rmse': float(xgb_train_rmse),
            'train_r2': float(xgb_train_r2),
            'test_rmse': float(xgb_test_rmse),
            'test_r2': float(xgb_test_r2)
        },
        'lgbm': {
            'train_rmse': float(lgbm_train_rmse),
            'train_r2': float(lgbm_train_r2),
            'test_rmse': float(lgbm_test_rmse),
            'test_r2': float(lgbm_test_r2)
        }
    },
    'ensemble_performance': {
        'train_rmse': float(ensemble_train_rmse),
        'train_mae': float(ensemble_train_mae),
        'train_r2': float(ensemble_train_r2),
        'train_directional_accuracy': float(train_dir_acc),
        'test_rmse': float(ensemble_test_rmse),
        'test_mae': float(ensemble_test_mae),
        'test_r2': float(ensemble_test_r2),
        'test_directional_accuracy': float(test_dir_acc)
    },
    'meta_learner': {
        'best_alpha': float(ridge_meta.alpha_),
        'coefficients': {name: float(coef) for name, coef in zip(component_names, coefficients)},
        'intercept': float(intercept),
        'normalized_weights': {name: float(weight) for name, weight in zip(component_names, normalized_weights)}
    },
    'comparison': {
        'production_ensemble_rmse': production_ensemble_rmse,
        'exp_001_rmse': exp_001_rmse,
        'xgb_opt_002_rmse': xgb_opt_002_rmse,
        'improvement_vs_exp001_pct': float(improvement_vs_exp001),
        'improvement_vs_xgb_pct': float(improvement_vs_xgb),
        'gap_to_production_pct': float(gap_to_production)
    },
    'success_criteria': {
        'target_rmse_threshold': 2.5,
        'target_r2_threshold': -5.0,
        'rmse_met': bool(ensemble_test_rmse < 2.5),
        'r2_met': bool(ensemble_test_r2 > -5.0),
        'overall_success': bool(success_criteria_met)
    }
}

if sarima_success:
    metadata['component_performance']['sarima'] = {
        'train_rmse': float(sarima_train_rmse),
        'train_r2': float(sarima_train_r2),
        'test_rmse': float(sarima_test_rmse),
        'test_r2': float(sarima_test_r2)
    }

if var_success:
    metadata['component_performance']['var'] = {
        'train_rmse': float(var_train_rmse),
        'train_r2': float(var_train_r2),
        'test_rmse': float(var_test_rmse),
        'test_r2': float(var_test_r2)
    }

with open(OUTPUT_PATH / f'{EXPERIMENT_ID}_metadata.json', 'w') as f:
    json.dump(metadata, f, indent=2)

# Save predictions
predictions_df = pd.DataFrame({
    'date': y_test.index,
    'actual': y_test.values,
    'ensemble': ensemble_test_pred,
    'xgb': xgb_test_pred,
    'lgbm': lgbm_test_pred
})

if sarima_success:
    predictions_df['sarima'] = sarima_test_pred.values

if var_success:
    predictions_df['var'] = var_test_pred.values

predictions_df.to_csv(OUTPUT_PATH / f'{EXPERIMENT_ID}_predictions.csv', index=False)

# Save feature stability
stability_df.to_csv(OUTPUT_PATH / f'{EXPERIMENT_ID}_feature_stability.csv', index=False)

print(f"\nModel artifacts saved:")
print(f"  - Component models: {EXPERIMENT_ID}_*_component.pkl")
print(f"  - Meta-learner: {EXPERIMENT_ID}_ridge_meta.pkl")
print(f"  - Features: {EXPERIMENT_ID}_features.json")
print(f"  - Metadata: {EXPERIMENT_ID}_metadata.json")
print(f"  - Predictions: {EXPERIMENT_ID}_predictions.csv")
print(f"  - Feature stability: {EXPERIMENT_ID}_feature_stability.csv")

# ============================================================================
# 12. Summary and Recommendations
# ============================================================================

print("\n" + "=" * 80)
print("12. SUMMARY AND RECOMMENDATIONS")
print("=" * 80)

print(f"\n‚úÖ ENSEMBLE-EXP-002 Training Complete!")
print(f"\nKey Results:")
print(f"  ‚Ä¢ Test RMSE: {ensemble_test_rmse:.4f} ({improvement_vs_exp001:+.1f}% vs EXP-001)")
print(f"  ‚Ä¢ Test R¬≤: {ensemble_test_r2:.4f}")
print(f"  ‚Ä¢ Directional Accuracy: {test_dir_acc:.1f}%")
print(f"  ‚Ä¢ Components: {len(component_names)} ({', '.join(component_names)})")
print(f"  ‚Ä¢ Features: {len(selected_features)} tiered features (vs 2 in EXP-001)")

print(f"\nFeature Distribution:")
tier1_count = len(stability_df[stability_df['tier'] == 1])
tier2_count = len(stability_df[stability_df['tier'] == 2])
tier3_count = len(stability_df[stability_df['tier'] == 3])
print(f"  ‚Ä¢ Tier 1 (p>0.05):  {tier1_count} features")
print(f"  ‚Ä¢ Tier 2 (p>0.01):  {tier2_count} features")
print(f"  ‚Ä¢ Tier 3 (p>0.001): {tier3_count} features")

print(f"\nMeta-Learner Insights:")
for name, coef, weight in zip(component_names, coefficients, normalized_weights):
    sign = "‚ö†Ô∏è  NEGATIVE" if coef < 0 else "‚úÖ POSITIVE"
    print(f"  ‚Ä¢ {name}: {coef:+.4f} ({weight:.1f}%) - {sign}")

if any(coefficients < 0):
    print(f"\n‚ö° Negative coefficients detected - ensemble using bias correction!")

print(f"\nNext Steps:")
if success_criteria_met:
    print(f"  ‚úÖ Success criteria met!")
    print(f"  ‚Üí Analyze component interactions and error patterns")
    print(f"  ‚Üí Test on out-of-time validation period")
    print(f"  ‚Üí Document findings and compare to production ensemble")
else:
    print(f"  ‚ö†Ô∏è  Partial success - further optimization possible")
    print(f"  ‚Üí Analyze which tier contributed most to improvement")
    print(f"  ‚Üí Consider adding external data sources")
    print(f"  ‚Üí Experiment with alternative meta-learners")

print("\n" + "=" * 80)
print("EXPERIMENT COMPLETE")
print("=" * 80)
